#  Household Electric Power Consumption  ( HEPC )

# 작업순서
import: 필요한 모듈 import
전처리: 학습에 필요한 데이터 전처리를 수행합니다.
모델링(model): 모델을 정의합니다.
컴파일(compile): 모델을 생성합니다.
학습 (fit): 모델을 학습시킵니다.

```python

# step1 필요한 모듈 임포트
import urllib
import os
import zipfile
import pandas as pd

import tensorflow as tf
from tensorflow.keras.layers import Dense, Conv1D, LSTM
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint

# step2 데이터 전처리
    # 데이터 가져오기
def download_and_extract_data():
    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'
    urllib.request.urlretrieve(url, 'household_power.zip')
    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:
        zip_ref.extractall()

download_and_extract_data()

df = pd.read_csv('household_power_consumption.csv', sep=',', infer_datetime_format=True, index_col='datetime', header=0)
df.head(10)

    # FEATURES에 데이터프레임의 Column 개수 대입
N_FEATURES = len(df.columns)

    # 데이터프레임을 numpy array으로 가져와 data에 대입
data = df.values

    # 전처리
def normalize_series(data, min, max):
    data = data - min
    data = data / max
    return data


    # 데이터 정규화
data = normalize_series(data, data.min(axis=0), data.max(axis=0))
data   # 결과 출력하여 확인

pd.DataFrame(data).describe()

    # 데이터셋 분할 (0.8). 
# 기존 0.5 -> 0.8로 변경 // 다른 비율로 변경 가능
split_time = int(len(data) * 0.8)
x_train = data[:split_time]
x_valid = data[split_time:]

    # 윈오두즈 데이터 생성
def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(size=(n_past + n_future), shift = shift, drop_remainder = True)
    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))
    ds = ds.shuffle(len(series))
    ds = ds.map(
        lambda w: (w[:n_past], w[n_past:])
    )
    return ds.batch(batch_size).prefetch(1)


BATCH_SIZE = 32 # 변경 가능하나 더 올리는 것은 비추 (내리는 것은 가능하나 시간 오래 걸림)
N_PAST = 24 # 변경 불가.
N_FUTURE = 24 # 변경 불가.
SHIFT = 1 # 변경 불가.

train_set = windowed_dataset(series=x_train, 
                             batch_size=BATCH_SIZE,
                             n_past=N_PAST, 
                             n_future=N_FUTURE,
                             shift=SHIFT)

valid_set = windowed_dataset(series=x_valid, 
                             batch_size=BATCH_SIZE,
                             n_past=N_PAST, 
                             n_future=N_FUTURE,
                             shift=SHIFT)

# step3 모델생성
model = tf.keras.models.Sequential([
    Conv1D(filters=32, 
            kernel_size=3,
            padding="causal",
            activation="relu",
            input_shape=[N_PAST, 7],
            ),
    LSTM(32, return_sequences=True),
    Dense(32, activation="relu"),
    Dense(16, activation="relu"),
    Dense(N_FEATURES)
])

    # 체크포인트 생성
checkpoint_path='model/my_checkpoint.ckpt'

checkpoint = ModelCheckpoint(checkpoint_path,
                             save_weights_only=True,
                             save_best_only=True,
                             monitor='val_loss',
                             verbose=1,
                             )



# learning_rate=0.0005, Adam 옵치마이저
optimizer =  tf.keras.optimizers.Adam(learning_rate=0.0005)  #기본 0.001

# step4 모델컴파일
model.compile(loss='mae',
              optimizer=optimizer,
              metrics=["mae"]
              )

# step5 모델학습
model.fit(train_set, 
        validation_data=(valid_set), 
        epochs=20, 
        callbacks=[checkpoint], 
        )
model.load_weights(checkpoint_path)

# step6 모델검증
model.evaluate(valid_set)

```
